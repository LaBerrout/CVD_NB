# -*- coding: utf-8 -*-
"""Copy of CVD Dataset Classifiers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vOFjshuGrqhnPqfoPHhCLJ-8eBN_Z1kP
"""

"""
Created on Tuesday April 27, 2024
Testing ML methods

@author: laberroutra
"""

import csv
import math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.model_selection import train_test_split
#from sklearn.naive_bayes import GaussianNB
#from sklearn.metrics import accuracy_score

"""## Naive Bayes Classifier

Functions

Main
"""

# Mean Function
def mean_col(df, column_name):
    if column_name in df.columns:
        return df[column_name].mean()
    else:
        raise ValueError(f"Column '{column_name}' does not exist in the DataFrame")

# Standard Deviation Function
def std_col(df, column_name):
    if column_name in df.columns:
        return df[column_name].std()
    else:
        raise ValueError(f"Column '{column_name}' does not exist in the DataFrame")

# Likelihood
def likelihood(row, str, mean, std):
    # row[str] is the x_value from the gaussian distribution for the value in the column str
    # Getting the y-axis value for the Gaussian distribution:
    # - norm.pdf computes the value of the probability density function for a
    # - normal distribution at x_value with the specified mean and standard deviation.
    return norm.pdf(row[str], loc=mean, scale=std)

# Get Score
def score_cn(row, prob, likelihoods):
    sum = np.log(prob)
    for i in range(len(likelihoods)):
        sum  += np.log(row[likelihoods[i]])
    return sum

# Compare scores
def comp_scores(row):
    if row['score_true'] > row['score_false']:
        return 1
    else:
        return 0

# True positive
def tp_comp(row):
    if row['target_col'] == 1:
        if row['target_col'] == row['score']:
            return 1
        else:
            return 0
    else:
        return 0

# False negative
def fp_comp(row):
    if row['score'] == 1:
        if row['target_col'] != row['score']:
            return 1
        else:
            return 0
    else:
        return 0

# True negative
def tn_comp(row):
    if row['score'] == 0:
        if row['target_col'] == row['score']:
            return 1
        else:
            return 0
    else:
        return 0

# False negative
def fn_comp(row):
    if row['score'] == 0:
        if row['target_col'] != row['score']:
            return 1
        else:
            return 0
    else:
        return 0

# Naive Bayes function: returns results of accuracy, precision, recall and F1-Score
def naive_bayes(df,cols_data,target_col,run_times):
  # Split the dataset to training and testing subdatasets
  df_train, df_test = train_test_split(df, test_size=0.2)

  # Prior probabilities
  total_points = df_train.shape[0]
  print('Total points:',total_points)

  prob_true = df_train[target_col].value_counts()[1] / total_points
  prob_false = df_train[target_col].value_counts()[0] / total_points
  print('True cases (with a cardio disease):', f"{prob_true:.2%}")
  print('False cases:', f"{prob_false:.2%}")

  df_true = df_train[df_train[target_col] == 1]
  df_false = df_train[df_train[target_col] == 0]

  # Is the training and testing datasets balanced?
  true_test_size = df_test[target_col].sum()
  true_train_size = df_train[target_col].sum()
  true_test_ratio = true_test_size/df_test.shape[0]
  true_train_ratio = true_train_size/df_train.shape[0]
  print('\nChecking both the datasets are balanced: (should be ~50%)')
  print('Total number of true cases of cardio diseases in the test dataset', '{:,}'.format(true_test_size), 'from', '{:,}'.format(df_test.shape[0]), ',', format(true_test_ratio, ".2%"), 'of the cases')
  print('Total number of true cases of cardio diseases in the train dataset', '{:,}'.format(true_train_size), 'from', '{:,}'.format(df_train.shape[0]), ',', format(true_train_ratio, ".2%"), 'of the cases')

  if (true_test_ratio > 0.45 and true_train_ratio > 0.45):
    print('The datasets are balanced')
  else:
    print('\nThe datasets are not balanced')

# *********** Main ***********
# Take the df resulting from my analysis
df = pd.read_csv('cardio_train_copy.csv', index_col=0)

print("Variables:")
for i in df:
    print(i)

# Have many people in the dataset
count_people = df.shape[0]
print("\nCount of people in the cohort:", count_people)

cols_data = ["smoke", "alco", "active"]
target_col = 'cardio'
run_times = 5

# call naive_bayes functions
results = naive_bayes(df,cols_data,target_col,run_times)

# Save the results of all the runs
all_runs_results = []
all_runs_results.append(results)

# Split the dataset to training and testing subdatasets
df_train, df_test = train_test_split(df, test_size=0.2)

# Prior probabilities
total_points = df_train.shape[0]
print('Total points:',total_points)

prob_true = df_train[target_col].value_counts()[1] / total_points
prob_false = df_train[target_col].value_counts()[0] / total_points
print('True cases (with a cardio disease):', f"{prob_true:.2%}")
print('False cases:', f"{prob_false:.2%}")

df_true = df_train[df_train[target_col] == 1]
df_false = df_train[df_train[target_col] == 0]

# Is the training and testing datasets balanced?
true_test_size = df_test[target_col].sum()
true_train_size = df_train[target_col].sum()
true_test_ratio = true_test_size/df_test.shape[0]
true_train_ratio = true_train_size/df_train.shape[0]
print('\nChecking both the datasets are balanced: (should be ~50%)')
print('Total number of true cases of cardio diseases in the test dataset', '{:,}'.format(true_test_size), 'from', '{:,}'.format(df_test.shape[0]), ',', format(true_test_ratio, ".2%"), 'of the cases')
print('Total number of true cases of cardio diseases in the train dataset', '{:,}'.format(true_train_size), 'from', '{:,}'.format(df_train.shape[0]), ',', format(true_train_ratio, ".2%"), 'of the cases')

if (true_test_ratio > 0.45 and true_train_ratio > 0.45):
  print('The datasets are balanced')
else:
  print('\nThe datasets are not balanced')

# Get the means and deviation standards for each variable to analyze (cols_data) for true and false cases
save_means_stds = []

for i in range(len(cols_data)):
  save_means_stds.append([mean_col(df_true, cols_data[i]),  std_col(df_true,cols_data[i]), mean_col(df_false, cols_data[i]),  std_col(df_false, cols_data[i])])

print("Mean True cases| STDV True cases | Mean False cases | STDV False cases")
for i in range(len(save_means_stds)):
  print(i,":",save_means_stds[i])

# Get the likelihoods for true and false cases
save_likelihoods_true = []
save_likelihoods_false = []

for i in range(len(cols_data)):
  save_likelihoods_true.append(df_test.apply(likelihood, axis=1, args=(cols_data[i], save_means_stds[i][0], save_means_stds[i][1])))
  save_likelihoods_false.append(df_test.apply(likelihood, axis=1, args=(cols_data[i], save_means_stds[i][2], save_means_stds[i][3])))

# Transponse the results for both TRUE and FALSE cases
df_lik_true = pd.DataFrame(save_likelihoods_true).T
df_lik_false = pd.DataFrame(save_likelihoods_false).T

cols_num = []   #Give a label to the columns in the df_lik dataframes
for i in range(len(cols_data)):
  cols_num.append(i)

# Get the score for each likelihood (TRUE and FALSE cases)
df_lik_true['score_true'] = df_lik_true.apply(score_cn, axis=1, args=(prob_true, cols_num))
df_lik_false['score_false'] = df_lik_false.apply(score_cn, axis=1, args=(prob_false, cols_num))

# Dataframe with all the scores and final score for each row
df_scores = pd.concat([df_lik_true, df_lik_false], axis=1)
df_scores['score'] = df_scores.apply(comp_scores, axis=1)

# Get the target col from the test dataset to the score dataframe df_score
df_scores['target_col'] = df_test[target_col]
df_scores.to_csv('scores.csv')

print(df_scores)



# ************************ Get True positives, False positives, True negatives and False negatives ************************
df_scores['TP'] = df_scores.apply(tp_comp, axis=1)
df_scores['FP'] = df_scores.apply(fp_comp, axis=1)
df_scores['TN'] = df_scores.apply(tn_comp, axis=1)
df_scores['FN'] = df_scores.apply(fn_comp, axis=1)

print(df_scores)

# ************************ Accuracy, Precision, Recall, F1 Score ************************
# Sum of TP, FP, TN, and FN
tp = df_scores['TP'].sum()
fp = df_scores['FP'].sum()
tn = df_scores['TN'].sum()
fn = df_scores['FN'].sum()

print('Results:')
# Accuracy
accuracy = (tp + tn) / (tp + tn + fp + fn)
print('  - Accuracy:', format(accuracy, ".2%"))

# Precision
precision = (tp) / (tp + fp)
print(' - Precision:', format(precision, ".2%"))

# Recall
recall = tp / (tp + fn)
print(' - Recall:', format(recall, ".2%"))

# F1-Score
f1score = 2 * ((precision * recall)/(precision + recall))
print(' - F-1 Score:', format(f1score, ".2%"))

results = [accuracy, precision, recall, f1score]

# Which of the analyzed columns is the most significant
def comp_lik(row, str1, str2):
    if row[str1] > row[str2]:
        return 1
    else:
        return 0

print('\nGetting the predominant column:')

# I want to know how much greater a likelihood for 1 is so much greater than 0 in the same column
# Feature 1
df_scores['lik1_wins_true'] = df_test.apply(comp_lik, axis=1, args=('Likelihood1c', 'Likelihood1n'))
wins1c = df_test['lik1_wins_true'].sum()
wins1n = df_test.shape[0] - wins1c
print('Feature 1:', str1)
print(' ','{:,}'.format(wins1c),'from',df_test.shape[0])
print(' ','{:,}'.format(wins1n),'from',df_test.shape[0])


# Feature 2
df_test['lik2_wins_c'] = df_test.apply(comp_lik, axis=1, args=('Likelihood2c', 'Likelihood2n'))
wins2c = df_test['lik2_wins_c'].sum()
wins2n = df_test.shape[0] - wins2c
print('\nFeature 2:', str2)
print(' ','{:,}'.format(wins2c),'from',df_test.shape[0])
print(' ','{:,}'.format(wins2n),'from',df_test.shape[0])

# Feature 3
df_test['lik3_wins_c'] = df_test.apply(comp_lik, axis=1, args=('Likelihood3c', 'Likelihood3n'))
wins3c = df_test['lik3_wins_c'].sum()
wins3n = df_test.shape[0] - wins3c
print('\nFeature 3:', str3)
print(' ','{:,}'.format(wins3c),'from',df_test.shape[0])
print(' ','{:,}'.format(wins3n),'from',df_test.shape[0])

#Add results to a dataframe
df_results = pd.DataFrame()

"""- **Accuracy**, is calculated as the sum of true positives and true negatives divided by the total number of samples.
- **Precision**, is calculated as the number of true positives divided by the sum of true positives and false positives.
- **Recall **,is calculated as the number of true positives divided by the sum of true positives and false negatives.
- **F1 Score **,is calculated as 2 * (Precision * Recall) / (Precision + Recall).

Accuracy, Precision, Recall, F1 Score



"""

# Naive Bayes Function (OLD)
def Naive_Bayes(name_dataset, target_col, array_cols):
    # df               : dataframe of the dataset
    # name_col_target  : name of the column of the target column
    # array_cols       : array of string of the columns (feature) to analyze

    # Create dataframe of the dataset to analyze
    df = pd.read_csv(name_dataset, index_col=0)

    # Add an ID column to the a

    # Obtain the total points in the dataset
    total_points = df_train.shape[0]

    # Obtain the training and test datasets
    df_train, df_test = train_test_split(df, test_size=0.2)

    # Prior probabilities
    # Probability of TRUE and FALSE cases in the target column
    prob_true = df_train[target_col].value_counts()[1] / total_points   # 1: true cases
    prob_false = df_train[target_col].value_counts()[0] / total_points  # 0: false cases

    # Obtain only the TRUE cases from the training set
    df_train_true = df_train[df_train[target_col] == 1]

    # Obtain only the FALSE cases from the training set
    df_train_false = df_train[df_train[target_col] == 0]

    # Obtain the mean and standard deviation for the TRUE and FALSE cases, for each feature
    mean_std_arr_true = []
    mean_std_arr_false = []

    for i in range(len(array_cols)):
        mean_std_arr_true.append(
            [mean_col(df_train_true, array_cols[i]), std_col(df_train_true, array_cols[i])]
        )
        mean_std_arr_false.append(
            [mean_col(df_train_false, array_cols[i]), std_col(df_train_false, array_cols[i])]
        )

    # Get the likelihoods for the TRUE and FALSE cases, for each feature
    df_likelihoods = pd.DataFrame()
    likelihoods_true = []
    likelihoods_false = []

    for i in range(len(array_cols)):
        str_col_true = "like_true" + str(i)
        df_likelihoods[str_col_true] = df_test.apply(
            likelihood,
            axis=1,
            args=(array_cols[i], mean_std_arr_true[i][0], mean_std_arr_true[i][1]),
        )
        likelihoods_true.append(str_col_true)

    for i in range(len(array_cols)):
        str_col_false = "like_false" + str(i)
        df_likelihoods[str_col_false] = df_test.apply(
            likelihood,
            axis=1,
            args=(array_cols[i], mean_std_arr_false[i][0], mean_std_arr_false[i][1]),
        )
        likelihoods_false.append(str_col_false)

    # Get the score from the likelihoods
    df_likelihoods["score_true"] = df_likelihoods.apply(
        score_cn, axis=1, args=(prob_true, likelihoods_true)
    )
    df_likelihoods["score_false"] = df_likelihoods.apply(
        score_cn, axis=1, args=(prob_false, likelihoods_false)
    )

    # Compare scores to determine if corresponds to a TRUE or FALSE case
    # Greater score determines if the case is TRUE
    df_likelihoods["score"] = df_likelihoods.apply(comp_scores, axis=1)